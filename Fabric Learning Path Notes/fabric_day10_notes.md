
# ğŸ“˜ Microsoft Fabric â€“ Study Notes (Day 10)

## ğŸ”· Module: Use Apache Spark in Microsoft Fabric  
ğŸ”— [Link to module](https://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/)

### ğŸ§  Key Concepts:
ğŸ”¹ **Apache Spark** notebooks are fully integrated into Microsoft Fabric and provide a powerful environment for data exploration, transformation, and analysis.  
ğŸ”¹ Spark notebooks support **PySpark**, **Spark SQL**, **Scala**, and **C#**, and can directly access data stored in **Lakehouse** using **OneLake**.

### ğŸ”§ What I practiced:
ğŸ”¹ Opened and explored a **Spark notebook**  
ğŸ”¹ Loaded data from a Lakehouse into a Spark DataFrame  
ğŸ”¹ Previewed, filtered, and transformed data using **PySpark** syntax  
ğŸ”¹ Saved transformed data back into the Lakehouse as managed tables  
ğŸ”¹ Used `display()` for inline visualization and result previewing  
ğŸ”¹ Ran SQL queries directly against Spark DataFrames

### âœ… Summary Takeaways:
ğŸ”¹ Microsoft Fabric makes it seamless to move between **code-first** and **visual** data workflows  
ğŸ”¹ Spark notebooks are a flexible and scalable way to work with big data  
ğŸ”¹ Integration with Lakehouse and OneLake reduces friction in the analytics pipeline  
ğŸ”¹ Even basic PySpark operations provide a lot of analytical power â€” and the experience feels natural for data professionals coming from Python/Pandas